{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys\n",
    "sys.path.append('./gcn')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from preprocess import*\n",
    "from torchdata import*\n",
    "from loss import*\n",
    "from network import*\n",
    "from gcn.models import GCN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "[ 5  6  8  9 11 12]\n",
      "[ 5  6  7  8  9 11 12]\n",
      "[ 2  5  6  7  8  9 10 11 12]\n",
      "[ 5  6  7 12]\n",
      "Dataset loaded!\n",
      "Model initialized!\n",
      "Statistics initialized!\n",
      "Optimizer defined!\n"
     ]
    }
   ],
   "source": [
    "solver = Solver( z_dim=32, encodeLayer=[256,64], decodeLayer=[64,256], nfeat=32, nclasses=6, \n",
    "                sigma=1/np.exp(4.5), entropy_thr=10, Lambda_global=200, Lambda_local=0.001, \n",
    "                beta=0.65, alpha=1.0, use_target=1, unas_thr=1.5, use_filter=False, filter_index=False,\n",
    "                ndomain=3, batch_size=256, highly_variable=2000, learning_rate=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:   0 [  1/1.3984375 (0.72%)]\tLoss_zinb: 1.92483\tLoss_cls_domain: 1.79981\tLoss_cls_source: 1.74309\tLoss_cls_target: 0.29537\tLoss_global: 142.60490\tLoss_local: 0.00000\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1017/2282 (44.5662%), Best Accuracy: 1017/2282 (44.5662%)  \n",
      "\n",
      "Train Epoch:   1 [  1/1.3984375 (0.72%)]\tLoss_zinb: 1.88081\tLoss_cls_domain: 1.91463\tLoss_cls_source: 1.64241\tLoss_cls_target: 0.29475\tLoss_global: 255.06796\tLoss_local: 0.00000\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1119/2282 (49.0359%), Best Accuracy: 1119/2282 (49.0359%)  \n",
      "\n",
      "Train Epoch:   2 [  1/1.3984375 (0.72%)]\tLoss_zinb: 1.87292\tLoss_cls_domain: 1.77878\tLoss_cls_source: 1.50904\tLoss_cls_target: 0.29312\tLoss_global: 266.20587\tLoss_local: 0.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7ff7efbf89e0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/msyuan/anaconda3/lib/python3.7/logging/__init__.py\", line 221, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 36347) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSocketClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mSocketClient\u001b[0;34m(address)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9ea973bcda6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_gcn_adapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/scMSDA/network.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, epoch, record_file, save_model)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scMSDA/torchdataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader_C_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mC\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mC1\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mC2\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mC3\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 36347) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "for t in range(200):\n",
    "    num = solver.train_gcn_adapt(t)\n",
    "    best_acc = solver.test(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre train Epoch:  0\n",
      "Train Epoch:   0 [  1/8.2890625 (0.12%)]\tLoss_zinb: 0.33845\tLoss_cls_domain: 2.24155\tLoss_cls_source: 2.13571\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   0 [  3/8.2890625 (0.36%)]\tLoss_zinb: 0.32878\tLoss_cls_domain: 2.04141\tLoss_cls_source: 1.82368\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   0 [  5/8.2890625 (0.60%)]\tLoss_zinb: 0.34077\tLoss_cls_domain: 1.77141\tLoss_cls_source: 1.23221\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   0 [  7/8.2890625 (0.84%)]\tLoss_zinb: 0.32649\tLoss_cls_domain: 1.41663\tLoss_cls_source: 0.80245\tLoss_global: nan\tLoss_local: 0.00001\n",
      "Train Epoch:   0 [  9/8.2890625 (1.09%)]\tLoss_zinb: 0.27687\tLoss_cls_domain: 1.11877\tLoss_cls_source: 0.48988\tLoss_global: nan\tLoss_local: 0.00001\n",
      "Train Epoch:   0 [ 11/8.2890625 (1.33%)]\tLoss_zinb: 0.23561\tLoss_cls_domain: 1.40439\tLoss_cls_source: 0.48313\tLoss_global: nan\tLoss_local: 0.00001\n",
      "Train Epoch:   0 [ 13/8.2890625 (1.57%)]\tLoss_zinb: 0.19727\tLoss_cls_domain: 1.03369\tLoss_cls_source: 0.41876\tLoss_global: nan\tLoss_local: 0.00001\n",
      "Train Epoch:   0 [ 15/8.2890625 (1.81%)]\tLoss_zinb: 0.19573\tLoss_cls_domain: 0.80967\tLoss_cls_source: 0.37881\tLoss_global: nan\tLoss_local: 0.00001\n",
      "Train Epoch:   0 [ 17/8.2890625 (2.05%)]\tLoss_zinb: 0.18614\tLoss_cls_domain: 1.10276\tLoss_cls_source: 0.29120\tLoss_global: nan\tLoss_local: 0.00001\n",
      "Train Epoch:   0 [ 19/8.2890625 (2.29%)]\tLoss_zinb: 0.17396\tLoss_cls_domain: 0.79496\tLoss_cls_source: 0.27535\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   0 [ 21/8.2890625 (2.53%)]\tLoss_zinb: 0.16414\tLoss_cls_domain: 1.05305\tLoss_cls_source: 0.21736\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   0 [ 23/8.2890625 (2.77%)]\tLoss_zinb: 0.16490\tLoss_cls_domain: 0.60483\tLoss_cls_source: 0.24911\tLoss_global: nan\tLoss_local: 0.00000\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1326/2122 (62.4882%), Best Accuracy: 1326/2122 (62.4882%)  \n",
      "\n",
      "Pre train Epoch:  1\n",
      "Train Epoch:   1 [  1/8.2890625 (0.12%)]\tLoss_zinb: 0.16341\tLoss_cls_domain: 0.60848\tLoss_cls_source: 0.24968\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   1 [  3/8.2890625 (0.36%)]\tLoss_zinb: 0.15755\tLoss_cls_domain: 0.79449\tLoss_cls_source: 0.16339\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   1 [  5/8.2890625 (0.60%)]\tLoss_zinb: 0.15543\tLoss_cls_domain: 0.86957\tLoss_cls_source: 0.24506\tLoss_global: nan\tLoss_local: 0.00001\n",
      "Train Epoch:   1 [  7/8.2890625 (0.84%)]\tLoss_zinb: 0.15675\tLoss_cls_domain: 0.72912\tLoss_cls_source: 0.13718\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   1 [  9/8.2890625 (1.09%)]\tLoss_zinb: 0.14906\tLoss_cls_domain: 0.56473\tLoss_cls_source: 0.14436\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   1 [ 11/8.2890625 (1.33%)]\tLoss_zinb: 0.15394\tLoss_cls_domain: 0.53093\tLoss_cls_source: 0.17116\tLoss_global: nan\tLoss_local: 0.00001\n",
      "Train Epoch:   1 [ 13/8.2890625 (1.57%)]\tLoss_zinb: 0.14630\tLoss_cls_domain: 0.54275\tLoss_cls_source: 0.17279\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   1 [ 15/8.2890625 (1.81%)]\tLoss_zinb: 0.14884\tLoss_cls_domain: 0.76769\tLoss_cls_source: 0.07969\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   1 [ 17/8.2890625 (2.05%)]\tLoss_zinb: 0.15185\tLoss_cls_domain: 0.35381\tLoss_cls_source: 0.12578\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   1 [ 19/8.2890625 (2.29%)]\tLoss_zinb: 0.15426\tLoss_cls_domain: 0.57411\tLoss_cls_source: 0.06478\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   1 [ 21/8.2890625 (2.53%)]\tLoss_zinb: 0.14098\tLoss_cls_domain: 0.54681\tLoss_cls_source: 0.13647\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   1 [ 23/8.2890625 (2.77%)]\tLoss_zinb: 0.14921\tLoss_cls_domain: 0.42810\tLoss_cls_source: 0.11971\tLoss_global: nan\tLoss_local: 0.00000\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1444/2122 (68.0490%), Best Accuracy: 1444/2122 (68.0490%)  \n",
      "\n",
      "Pre train Epoch:  2\n",
      "Train Epoch:   2 [  1/8.2890625 (0.12%)]\tLoss_zinb: 0.15845\tLoss_cls_domain: 0.44088\tLoss_cls_source: 0.06894\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   2 [  3/8.2890625 (0.36%)]\tLoss_zinb: 0.15457\tLoss_cls_domain: 0.72232\tLoss_cls_source: 0.10132\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   2 [  5/8.2890625 (0.60%)]\tLoss_zinb: 0.14560\tLoss_cls_domain: 0.40665\tLoss_cls_source: 0.09980\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   2 [  7/8.2890625 (0.84%)]\tLoss_zinb: 0.14558\tLoss_cls_domain: 0.35721\tLoss_cls_source: 0.07204\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   2 [  9/8.2890625 (1.09%)]\tLoss_zinb: 0.15729\tLoss_cls_domain: 0.42992\tLoss_cls_source: 0.11583\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   2 [ 11/8.2890625 (1.33%)]\tLoss_zinb: 0.14690\tLoss_cls_domain: 0.43836\tLoss_cls_source: 0.10231\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   2 [ 13/8.2890625 (1.57%)]\tLoss_zinb: 0.14976\tLoss_cls_domain: 0.33608\tLoss_cls_source: 0.07758\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   2 [ 15/8.2890625 (1.81%)]\tLoss_zinb: 0.14240\tLoss_cls_domain: 0.35787\tLoss_cls_source: 0.06399\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   2 [ 17/8.2890625 (2.05%)]\tLoss_zinb: 0.14054\tLoss_cls_domain: 0.31598\tLoss_cls_source: 0.05491\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   2 [ 19/8.2890625 (2.29%)]\tLoss_zinb: 0.14408\tLoss_cls_domain: 0.34753\tLoss_cls_source: 0.08640\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   2 [ 21/8.2890625 (2.53%)]\tLoss_zinb: 0.15552\tLoss_cls_domain: 0.26894\tLoss_cls_source: 0.04995\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   2 [ 23/8.2890625 (2.77%)]\tLoss_zinb: 0.13973\tLoss_cls_domain: 0.60944\tLoss_cls_source: 0.08347\tLoss_global: nan\tLoss_local: 0.00001\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1386/2122 (65.3157%), Best Accuracy: 1444/2122 (68.0490%)  \n",
      "\n",
      "Pre train Epoch:  3\n",
      "Train Epoch:   3 [  1/8.2890625 (0.12%)]\tLoss_zinb: 0.13577\tLoss_cls_domain: 0.22737\tLoss_cls_source: 0.10621\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   3 [  3/8.2890625 (0.36%)]\tLoss_zinb: 0.15117\tLoss_cls_domain: 0.34171\tLoss_cls_source: 0.06243\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   3 [  5/8.2890625 (0.60%)]\tLoss_zinb: 0.15493\tLoss_cls_domain: 0.51264\tLoss_cls_source: 0.05178\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   3 [  7/8.2890625 (0.84%)]\tLoss_zinb: 0.14414\tLoss_cls_domain: 0.53421\tLoss_cls_source: 0.06237\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   3 [  9/8.2890625 (1.09%)]\tLoss_zinb: 0.14390\tLoss_cls_domain: 0.44065\tLoss_cls_source: 0.04135\tLoss_global: nan\tLoss_local: 0.00001\n",
      "Train Epoch:   3 [ 11/8.2890625 (1.33%)]\tLoss_zinb: 0.14906\tLoss_cls_domain: 0.27385\tLoss_cls_source: 0.08940\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   3 [ 13/8.2890625 (1.57%)]\tLoss_zinb: 0.14992\tLoss_cls_domain: 0.62626\tLoss_cls_source: 0.04556\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   3 [ 15/8.2890625 (1.81%)]\tLoss_zinb: 0.14505\tLoss_cls_domain: 0.28253\tLoss_cls_source: 0.05785\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   3 [ 17/8.2890625 (2.05%)]\tLoss_zinb: 0.14735\tLoss_cls_domain: 0.22680\tLoss_cls_source: 0.06516\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   3 [ 19/8.2890625 (2.29%)]\tLoss_zinb: 0.15348\tLoss_cls_domain: 0.20058\tLoss_cls_source: 0.05101\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   3 [ 21/8.2890625 (2.53%)]\tLoss_zinb: 0.15173\tLoss_cls_domain: 0.28672\tLoss_cls_source: 0.04690\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   3 [ 23/8.2890625 (2.77%)]\tLoss_zinb: 0.14887\tLoss_cls_domain: 0.17937\tLoss_cls_source: 0.03926\tLoss_global: nan\tLoss_local: 0.00000\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1300/2122 (61.2630%), Best Accuracy: 1444/2122 (68.0490%)  \n",
      "\n",
      "Pre train Epoch:  4\n",
      "Train Epoch:   4 [  1/8.2890625 (0.12%)]\tLoss_zinb: 0.14608\tLoss_cls_domain: 0.19079\tLoss_cls_source: 0.03292\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   4 [  3/8.2890625 (0.36%)]\tLoss_zinb: 0.15325\tLoss_cls_domain: 0.20632\tLoss_cls_source: 0.06038\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   4 [  5/8.2890625 (0.60%)]\tLoss_zinb: 0.14288\tLoss_cls_domain: 0.17503\tLoss_cls_source: 0.05592\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   4 [  7/8.2890625 (0.84%)]\tLoss_zinb: 0.14342\tLoss_cls_domain: 0.27756\tLoss_cls_source: 0.03353\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   4 [  9/8.2890625 (1.09%)]\tLoss_zinb: 0.14220\tLoss_cls_domain: 0.20680\tLoss_cls_source: 0.04642\tLoss_global: nan\tLoss_local: 0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:   4 [ 11/8.2890625 (1.33%)]\tLoss_zinb: 0.15068\tLoss_cls_domain: 0.33734\tLoss_cls_source: 0.05608\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   4 [ 13/8.2890625 (1.57%)]\tLoss_zinb: 0.14785\tLoss_cls_domain: 0.16144\tLoss_cls_source: 0.06124\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   4 [ 15/8.2890625 (1.81%)]\tLoss_zinb: 0.14353\tLoss_cls_domain: 0.21384\tLoss_cls_source: 0.04178\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   4 [ 17/8.2890625 (2.05%)]\tLoss_zinb: 0.15082\tLoss_cls_domain: 0.16797\tLoss_cls_source: 0.03640\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   4 [ 19/8.2890625 (2.29%)]\tLoss_zinb: 0.14464\tLoss_cls_domain: 0.18552\tLoss_cls_source: 0.03503\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   4 [ 21/8.2890625 (2.53%)]\tLoss_zinb: 0.14562\tLoss_cls_domain: 0.15523\tLoss_cls_source: 0.05394\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   4 [ 23/8.2890625 (2.77%)]\tLoss_zinb: 0.14551\tLoss_cls_domain: 0.16599\tLoss_cls_source: 0.03358\tLoss_global: nan\tLoss_local: 0.00000\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1126/2122 (53.0631%), Best Accuracy: 1444/2122 (68.0490%)  \n",
      "\n",
      "Pre train Epoch:  5\n",
      "Train Epoch:   5 [  1/8.2890625 (0.12%)]\tLoss_zinb: 0.14698\tLoss_cls_domain: 0.17718\tLoss_cls_source: 0.02559\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   5 [  3/8.2890625 (0.36%)]\tLoss_zinb: 0.14585\tLoss_cls_domain: 0.15596\tLoss_cls_source: 0.05007\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   5 [  5/8.2890625 (0.60%)]\tLoss_zinb: 0.13973\tLoss_cls_domain: 0.20797\tLoss_cls_source: 0.01908\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   5 [  7/8.2890625 (0.84%)]\tLoss_zinb: 0.15952\tLoss_cls_domain: 0.33285\tLoss_cls_source: 0.03529\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   5 [  9/8.2890625 (1.09%)]\tLoss_zinb: 0.14343\tLoss_cls_domain: 0.13464\tLoss_cls_source: 0.03841\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   5 [ 11/8.2890625 (1.33%)]\tLoss_zinb: 0.14177\tLoss_cls_domain: 0.13561\tLoss_cls_source: 0.02702\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   5 [ 13/8.2890625 (1.57%)]\tLoss_zinb: 0.14361\tLoss_cls_domain: 0.15901\tLoss_cls_source: 0.05224\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   5 [ 15/8.2890625 (1.81%)]\tLoss_zinb: 0.14552\tLoss_cls_domain: 0.33010\tLoss_cls_source: 0.03057\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   5 [ 17/8.2890625 (2.05%)]\tLoss_zinb: 0.14703\tLoss_cls_domain: 0.24532\tLoss_cls_source: 0.03021\tLoss_global: nan\tLoss_local: 0.00001\n",
      "Train Epoch:   5 [ 19/8.2890625 (2.29%)]\tLoss_zinb: 0.14560\tLoss_cls_domain: 0.34790\tLoss_cls_source: 0.04014\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   5 [ 21/8.2890625 (2.53%)]\tLoss_zinb: 0.14526\tLoss_cls_domain: 0.13414\tLoss_cls_source: 0.03138\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   5 [ 23/8.2890625 (2.77%)]\tLoss_zinb: 0.13984\tLoss_cls_domain: 0.16806\tLoss_cls_source: 0.01731\tLoss_global: nan\tLoss_local: 0.00000\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1008/2122 (47.5024%), Best Accuracy: 1444/2122 (68.0490%)  \n",
      "\n",
      "Pre train Epoch:  6\n",
      "Train Epoch:   6 [  1/8.2890625 (0.12%)]\tLoss_zinb: 0.15066\tLoss_cls_domain: 0.18713\tLoss_cls_source: 0.04947\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   6 [  3/8.2890625 (0.36%)]\tLoss_zinb: 0.14420\tLoss_cls_domain: 0.23727\tLoss_cls_source: 0.03619\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   6 [  5/8.2890625 (0.60%)]\tLoss_zinb: 0.14742\tLoss_cls_domain: 0.14428\tLoss_cls_source: 0.02140\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   6 [  7/8.2890625 (0.84%)]\tLoss_zinb: 0.14206\tLoss_cls_domain: 0.15267\tLoss_cls_source: 0.03215\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   6 [  9/8.2890625 (1.09%)]\tLoss_zinb: 0.14321\tLoss_cls_domain: 0.13327\tLoss_cls_source: 0.02842\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   6 [ 11/8.2890625 (1.33%)]\tLoss_zinb: 0.13641\tLoss_cls_domain: 0.18375\tLoss_cls_source: 0.02635\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   6 [ 13/8.2890625 (1.57%)]\tLoss_zinb: 0.14587\tLoss_cls_domain: 0.14961\tLoss_cls_source: 0.02778\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   6 [ 15/8.2890625 (1.81%)]\tLoss_zinb: 0.15248\tLoss_cls_domain: 0.13931\tLoss_cls_source: 0.03424\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   6 [ 17/8.2890625 (2.05%)]\tLoss_zinb: 0.14546\tLoss_cls_domain: 0.16198\tLoss_cls_source: 0.02888\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   6 [ 19/8.2890625 (2.29%)]\tLoss_zinb: 0.14196\tLoss_cls_domain: 0.17013\tLoss_cls_source: 0.05002\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   6 [ 21/8.2890625 (2.53%)]\tLoss_zinb: 0.14553\tLoss_cls_domain: 0.17091\tLoss_cls_source: 0.03476\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   6 [ 23/8.2890625 (2.77%)]\tLoss_zinb: 0.14416\tLoss_cls_domain: 0.15299\tLoss_cls_source: 0.02948\tLoss_global: nan\tLoss_local: 0.00000\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 851/2122 (40.1037%), Best Accuracy: 1444/2122 (68.0490%)  \n",
      "\n",
      "Pre train Epoch:  7\n",
      "Train Epoch:   7 [  1/8.2890625 (0.12%)]\tLoss_zinb: 0.14626\tLoss_cls_domain: 0.20760\tLoss_cls_source: 0.04976\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   7 [  3/8.2890625 (0.36%)]\tLoss_zinb: 0.14654\tLoss_cls_domain: 0.13449\tLoss_cls_source: 0.03268\tLoss_global: nan\tLoss_local: 0.00001\n",
      "Train Epoch:   7 [  5/8.2890625 (0.60%)]\tLoss_zinb: 0.13984\tLoss_cls_domain: 0.14685\tLoss_cls_source: 0.02277\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   7 [  7/8.2890625 (0.84%)]\tLoss_zinb: 0.14395\tLoss_cls_domain: 0.13602\tLoss_cls_source: 0.02405\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   7 [  9/8.2890625 (1.09%)]\tLoss_zinb: 0.13982\tLoss_cls_domain: 0.19262\tLoss_cls_source: 0.02155\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   7 [ 11/8.2890625 (1.33%)]\tLoss_zinb: 0.14664\tLoss_cls_domain: 0.11030\tLoss_cls_source: 0.01305\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   7 [ 13/8.2890625 (1.57%)]\tLoss_zinb: 0.14453\tLoss_cls_domain: 0.10505\tLoss_cls_source: 0.02801\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   7 [ 15/8.2890625 (1.81%)]\tLoss_zinb: 0.14365\tLoss_cls_domain: 0.13859\tLoss_cls_source: 0.02851\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   7 [ 17/8.2890625 (2.05%)]\tLoss_zinb: 0.14529\tLoss_cls_domain: 0.17085\tLoss_cls_source: 0.04411\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   7 [ 19/8.2890625 (2.29%)]\tLoss_zinb: 0.14868\tLoss_cls_domain: 0.12618\tLoss_cls_source: 0.03545\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   7 [ 21/8.2890625 (2.53%)]\tLoss_zinb: 0.14237\tLoss_cls_domain: 0.32882\tLoss_cls_source: 0.02418\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   7 [ 23/8.2890625 (2.77%)]\tLoss_zinb: 0.14712\tLoss_cls_domain: 0.19164\tLoss_cls_source: 0.02250\tLoss_global: nan\tLoss_local: 0.00000\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 819/2122 (38.5957%), Best Accuracy: 1444/2122 (68.0490%)  \n",
      "\n",
      "Pre train Epoch:  8\n",
      "Train Epoch:   8 [  1/8.2890625 (0.12%)]\tLoss_zinb: 0.14354\tLoss_cls_domain: 0.21728\tLoss_cls_source: 0.02072\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   8 [  3/8.2890625 (0.36%)]\tLoss_zinb: 0.13443\tLoss_cls_domain: 0.13951\tLoss_cls_source: 0.02993\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   8 [  5/8.2890625 (0.60%)]\tLoss_zinb: 0.13794\tLoss_cls_domain: 0.10707\tLoss_cls_source: 0.03308\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   8 [  7/8.2890625 (0.84%)]\tLoss_zinb: 0.14961\tLoss_cls_domain: 0.11019\tLoss_cls_source: 0.03349\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   8 [  9/8.2890625 (1.09%)]\tLoss_zinb: 0.14026\tLoss_cls_domain: 0.25949\tLoss_cls_source: 0.02410\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   8 [ 11/8.2890625 (1.33%)]\tLoss_zinb: 0.13948\tLoss_cls_domain: 0.12774\tLoss_cls_source: 0.01631\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   8 [ 13/8.2890625 (1.57%)]\tLoss_zinb: 0.15323\tLoss_cls_domain: 0.09130\tLoss_cls_source: 0.01641\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   8 [ 15/8.2890625 (1.81%)]\tLoss_zinb: 0.14512\tLoss_cls_domain: 0.11717\tLoss_cls_source: 0.04645\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   8 [ 17/8.2890625 (2.05%)]\tLoss_zinb: 0.14394\tLoss_cls_domain: 0.07943\tLoss_cls_source: 0.02708\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   8 [ 19/8.2890625 (2.29%)]\tLoss_zinb: 0.14355\tLoss_cls_domain: 0.11463\tLoss_cls_source: 0.03415\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   8 [ 21/8.2890625 (2.53%)]\tLoss_zinb: 0.14127\tLoss_cls_domain: 0.12503\tLoss_cls_source: 0.04765\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   8 [ 23/8.2890625 (2.77%)]\tLoss_zinb: 0.14347\tLoss_cls_domain: 0.08869\tLoss_cls_source: 0.00961\tLoss_global: nan\tLoss_local: 0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 954/2122 (44.9576%), Best Accuracy: 1444/2122 (68.0490%)  \n",
      "\n",
      "Pre train Epoch:  9\n",
      "Train Epoch:   9 [  1/8.2890625 (0.12%)]\tLoss_zinb: 0.14567\tLoss_cls_domain: 0.10590\tLoss_cls_source: 0.02829\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   9 [  3/8.2890625 (0.36%)]\tLoss_zinb: 0.13253\tLoss_cls_domain: 0.08370\tLoss_cls_source: 0.02189\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   9 [  5/8.2890625 (0.60%)]\tLoss_zinb: 0.14992\tLoss_cls_domain: 0.10341\tLoss_cls_source: 0.02324\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   9 [  7/8.2890625 (0.84%)]\tLoss_zinb: 0.15067\tLoss_cls_domain: 0.09602\tLoss_cls_source: 0.02340\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   9 [  9/8.2890625 (1.09%)]\tLoss_zinb: 0.15492\tLoss_cls_domain: 0.10619\tLoss_cls_source: 0.05978\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   9 [ 11/8.2890625 (1.33%)]\tLoss_zinb: 0.14291\tLoss_cls_domain: 0.16393\tLoss_cls_source: 0.02744\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   9 [ 13/8.2890625 (1.57%)]\tLoss_zinb: 0.14631\tLoss_cls_domain: 0.08837\tLoss_cls_source: 0.07149\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   9 [ 15/8.2890625 (1.81%)]\tLoss_zinb: 0.13956\tLoss_cls_domain: 0.13265\tLoss_cls_source: 0.02326\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   9 [ 17/8.2890625 (2.05%)]\tLoss_zinb: 0.14865\tLoss_cls_domain: 0.18488\tLoss_cls_source: 0.03671\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   9 [ 19/8.2890625 (2.29%)]\tLoss_zinb: 0.14603\tLoss_cls_domain: 0.06881\tLoss_cls_source: 0.03215\tLoss_global: nan\tLoss_local: 0.00001\n",
      "Train Epoch:   9 [ 21/8.2890625 (2.53%)]\tLoss_zinb: 0.14150\tLoss_cls_domain: 0.07747\tLoss_cls_source: 0.04452\tLoss_global: nan\tLoss_local: 0.00000\n",
      "Train Epoch:   9 [ 23/8.2890625 (2.77%)]\tLoss_zinb: 0.14640\tLoss_cls_domain: 0.13845\tLoss_cls_source: 0.04376\tLoss_global: nan\tLoss_local: 0.00000\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1328/2122 (62.5825%), Best Accuracy: 1444/2122 (68.0490%)  \n",
      "\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "[ 2  5  6  7  8  9 10 11 12]\n",
      "8066\n",
      "8066\n",
      "[1.6589841e+00 2.9291990e-03 1.4308311e+00 ... 8.2692073e-05 3.8613567e-01\n",
      " 1.5538461e-03]\n"
     ]
    }
   ],
   "source": [
    "for t in range(10):\n",
    "    print('Pre train Epoch: ', t)\n",
    "    num = solver.train_gcn_baseline(t)\n",
    "    best_acc = solver.test(t)\n",
    "index = solver.open_filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1504\n",
      "448\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "X_all, label_all = prepro(\"Baron_human+Muraro\")\n",
    "X1, label1 = prepro(\"Baron_human\")\n",
    "X2, label2 = prepro(\"Muraro\")\n",
    "len_1,len_2 = len(label1),len(label2) \n",
    "label1 = label_all[:-len_2]\n",
    "label2 = label_all[-len_2:]\n",
    "print(len(index))\n",
    "print(np.sum([label2==[12]]))\n",
    "print(np.sum([label2[index]==[12]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1    2    4 ... 2119 2120 2121]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "[ 2  5  6  7  8  9 10 11 12]\n",
      "8066\n",
      "8066\n",
      "Filtered dataset loaded!\n",
      "Model initialized!\n",
      "Statistics initialized!\n",
      "Optimizer defined!\n"
     ]
    }
   ],
   "source": [
    "solver = Solver( z_dim=32, encodeLayer=[256,64], decodeLayer=[64,256], nfeat=32, nclasses=8, \n",
    "                sigma=1/np.exp(4.5), entropy_thr=10, Lambda_global=20, Lambda_local=0.001, \n",
    "                beta=0.65, alpha=1.0, use_target=1, unas_thr=3e-2, use_filter=True, filter_index=index,\n",
    "                ndomain=2, batch_size=256, highly_variable=2000, learning_rate=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Train Epoch:   0 [  1/7.80859375 (0.13%)]\tLoss_zinb: 1.10631\tLoss_cls_domain: 2.09772\tLoss_cls_source: 2.06230\tLoss_cls_target: 0.25766\tLoss_global: 16.99969\tLoss_local: 0.00000\n",
      "Train Epoch:   0 [  3/7.80859375 (0.38%)]\tLoss_zinb: 1.12130\tLoss_cls_domain: 2.02687\tLoss_cls_source: 1.70743\tLoss_cls_target: 0.25632\tLoss_global: 26.66663\tLoss_local: 0.00000\n",
      "Train Epoch:   0 [  5/7.80859375 (0.64%)]\tLoss_zinb: 1.04514\tLoss_cls_domain: 1.91613\tLoss_cls_source: 1.30248\tLoss_cls_target: 0.24748\tLoss_global: 26.66808\tLoss_local: 0.00000\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1236/2272 (54.4014%), Best Accuracy: 1236/2272 (54.4014%)  \n",
      "\n",
      "Epoch:  1\n",
      "Train Epoch:   1 [  1/7.80859375 (0.13%)]\tLoss_zinb: 1.02510\tLoss_cls_domain: 1.84778\tLoss_cls_source: 1.20843\tLoss_cls_target: 0.23241\tLoss_global: 26.66708\tLoss_local: 0.00000\n",
      "Train Epoch:   1 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.86247\tLoss_cls_domain: 1.73583\tLoss_cls_source: 1.02623\tLoss_cls_target: 0.19672\tLoss_global: 26.68793\tLoss_local: 0.00002\n",
      "Train Epoch:   1 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.80051\tLoss_cls_domain: 1.65940\tLoss_cls_source: 0.77623\tLoss_cls_target: 0.15031\tLoss_global: 26.65383\tLoss_local: 0.00001\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1503/2272 (66.1532%), Best Accuracy: 1503/2272 (66.1532%)  \n",
      "\n",
      "Epoch:  2\n",
      "Train Epoch:   2 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.77021\tLoss_cls_domain: 1.83170\tLoss_cls_source: 0.72210\tLoss_cls_target: 0.12553\tLoss_global: 26.67776\tLoss_local: 0.00001\n",
      "Train Epoch:   2 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.75677\tLoss_cls_domain: 1.94512\tLoss_cls_source: 0.61850\tLoss_cls_target: 0.09433\tLoss_global: 26.65104\tLoss_local: 0.00002\n",
      "Train Epoch:   2 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.66991\tLoss_cls_domain: 1.79779\tLoss_cls_source: 0.51368\tLoss_cls_target: 0.07744\tLoss_global: 26.63143\tLoss_local: 0.00001\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1551/2272 (68.2658%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  3\n",
      "Train Epoch:   3 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.65935\tLoss_cls_domain: 1.98087\tLoss_cls_source: 0.43354\tLoss_cls_target: 0.07571\tLoss_global: 26.72858\tLoss_local: 0.00001\n",
      "Train Epoch:   3 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.66665\tLoss_cls_domain: 2.38479\tLoss_cls_source: 0.41752\tLoss_cls_target: 0.05918\tLoss_global: 26.61211\tLoss_local: 0.00001\n",
      "Train Epoch:   3 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.64484\tLoss_cls_domain: 1.60717\tLoss_cls_source: 0.28092\tLoss_cls_target: 0.04600\tLoss_global: 26.87340\tLoss_local: 0.00002\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1517/2272 (66.7694%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  4\n",
      "Train Epoch:   4 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.62784\tLoss_cls_domain: 2.47413\tLoss_cls_source: 0.24948\tLoss_cls_target: 0.03909\tLoss_global: 26.49695\tLoss_local: 0.00003\n",
      "Train Epoch:   4 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.61086\tLoss_cls_domain: 4.14289\tLoss_cls_source: 0.28242\tLoss_cls_target: 0.03147\tLoss_global: 27.03637\tLoss_local: 0.00003\n",
      "Train Epoch:   4 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.60627\tLoss_cls_domain: 3.82756\tLoss_cls_source: 0.25837\tLoss_cls_target: 0.03029\tLoss_global: 26.86312\tLoss_local: 0.00001\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1543/2272 (67.9137%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  5\n",
      "Train Epoch:   5 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.61421\tLoss_cls_domain: 3.07493\tLoss_cls_source: 0.15262\tLoss_cls_target: 0.03252\tLoss_global: 26.85027\tLoss_local: 0.00002\n",
      "Train Epoch:   5 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.55716\tLoss_cls_domain: 2.94997\tLoss_cls_source: 0.20109\tLoss_cls_target: 0.02781\tLoss_global: 26.58271\tLoss_local: 0.00003\n",
      "Train Epoch:   5 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.56688\tLoss_cls_domain: 2.42781\tLoss_cls_source: 0.29392\tLoss_cls_target: 0.02781\tLoss_global: 26.88643\tLoss_local: 0.00002\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1532/2272 (67.4296%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  6\n",
      "Train Epoch:   6 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.55736\tLoss_cls_domain: 1.37146\tLoss_cls_source: 0.28682\tLoss_cls_target: 0.02997\tLoss_global: 27.19108\tLoss_local: 0.00003\n",
      "Train Epoch:   6 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.56304\tLoss_cls_domain: 2.17611\tLoss_cls_source: 0.22623\tLoss_cls_target: 0.03000\tLoss_global: 27.50250\tLoss_local: 0.00002\n",
      "Train Epoch:   6 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.58058\tLoss_cls_domain: 1.77651\tLoss_cls_source: 0.19650\tLoss_cls_target: 0.02822\tLoss_global: 27.01707\tLoss_local: 0.00003\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1516/2272 (66.7254%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  7\n",
      "Train Epoch:   7 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.55385\tLoss_cls_domain: 0.90706\tLoss_cls_source: 0.16157\tLoss_cls_target: 0.03158\tLoss_global: 28.52402\tLoss_local: 0.00004\n",
      "Train Epoch:   7 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.53177\tLoss_cls_domain: 1.23061\tLoss_cls_source: 0.18782\tLoss_cls_target: 0.02437\tLoss_global: 26.80789\tLoss_local: 0.00004\n",
      "Train Epoch:   7 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.52991\tLoss_cls_domain: 1.35944\tLoss_cls_source: 0.23126\tLoss_cls_target: 0.03045\tLoss_global: 26.34410\tLoss_local: 0.00003\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1527/2272 (67.2095%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  8\n",
      "Train Epoch:   8 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.52298\tLoss_cls_domain: 1.08216\tLoss_cls_source: 0.18836\tLoss_cls_target: 0.02738\tLoss_global: 28.00455\tLoss_local: 0.00004\n",
      "Train Epoch:   8 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.53442\tLoss_cls_domain: 0.92337\tLoss_cls_source: 0.24193\tLoss_cls_target: 0.03153\tLoss_global: 26.88259\tLoss_local: 0.00005\n",
      "Train Epoch:   8 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.53849\tLoss_cls_domain: 1.22160\tLoss_cls_source: 0.20635\tLoss_cls_target: 0.03563\tLoss_global: 28.83556\tLoss_local: 0.00004\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1489/2272 (65.5370%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  9\n",
      "Train Epoch:   9 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.52998\tLoss_cls_domain: 1.79252\tLoss_cls_source: 0.19384\tLoss_cls_target: 0.03255\tLoss_global: 27.02305\tLoss_local: 0.00005\n",
      "Train Epoch:   9 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.54853\tLoss_cls_domain: 1.28606\tLoss_cls_source: 0.14406\tLoss_cls_target: 0.03367\tLoss_global: 30.34056\tLoss_local: 0.00018\n",
      "Train Epoch:   9 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.53825\tLoss_cls_domain: 0.69405\tLoss_cls_source: 0.18263\tLoss_cls_target: 0.02976\tLoss_global: 27.53390\tLoss_local: 0.00015\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1475/2272 (64.9208%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  10\n",
      "Train Epoch:  10 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.53726\tLoss_cls_domain: 1.89401\tLoss_cls_source: 0.24509\tLoss_cls_target: 0.03254\tLoss_global: 27.01308\tLoss_local: 0.00005\n",
      "Train Epoch:  10 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.51034\tLoss_cls_domain: 0.79701\tLoss_cls_source: 0.16096\tLoss_cls_target: 0.02981\tLoss_global: 29.03182\tLoss_local: 0.00019\n",
      "Train Epoch:  10 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.51138\tLoss_cls_domain: 1.42463\tLoss_cls_source: 0.16962\tLoss_cls_target: 0.02753\tLoss_global: 29.95884\tLoss_local: 0.00011\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1486/2272 (65.4049%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  11\n",
      "Train Epoch:  11 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.52995\tLoss_cls_domain: 1.14035\tLoss_cls_source: 0.15638\tLoss_cls_target: 0.02353\tLoss_global: 25.92934\tLoss_local: 0.00010\n",
      "Train Epoch:  11 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.52935\tLoss_cls_domain: 1.06508\tLoss_cls_source: 0.17589\tLoss_cls_target: 0.03210\tLoss_global: 28.52947\tLoss_local: 0.00010\n",
      "Train Epoch:  11 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.51548\tLoss_cls_domain: 0.62758\tLoss_cls_source: 0.26255\tLoss_cls_target: 0.03416\tLoss_global: 29.26895\tLoss_local: 0.00010\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1468/2272 (64.6127%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  12\n",
      "Train Epoch:  12 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.53317\tLoss_cls_domain: 2.55955\tLoss_cls_source: 0.26930\tLoss_cls_target: 0.03662\tLoss_global: 29.00656\tLoss_local: 0.00013\n",
      "Train Epoch:  12 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.51981\tLoss_cls_domain: 1.60146\tLoss_cls_source: 0.22689\tLoss_cls_target: 0.04402\tLoss_global: 29.69424\tLoss_local: 0.00006\n",
      "Train Epoch:  12 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.51120\tLoss_cls_domain: 0.93312\tLoss_cls_source: 0.18912\tLoss_cls_target: 0.03559\tLoss_global: 29.20548\tLoss_local: 0.00012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1445/2272 (63.6004%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  13\n",
      "Train Epoch:  13 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.52324\tLoss_cls_domain: 1.30916\tLoss_cls_source: 0.18297\tLoss_cls_target: 0.04370\tLoss_global: 25.35595\tLoss_local: 0.00007\n",
      "Train Epoch:  13 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.52681\tLoss_cls_domain: 0.72489\tLoss_cls_source: 0.23645\tLoss_cls_target: 0.04263\tLoss_global: 31.30785\tLoss_local: 0.00005\n",
      "Train Epoch:  13 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.53042\tLoss_cls_domain: 0.79789\tLoss_cls_source: 0.24377\tLoss_cls_target: 0.03640\tLoss_global: 32.37571\tLoss_local: 0.00007\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1456/2272 (64.0845%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  14\n",
      "Train Epoch:  14 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.52597\tLoss_cls_domain: 1.66443\tLoss_cls_source: 0.15876\tLoss_cls_target: 0.03515\tLoss_global: 39.27097\tLoss_local: 0.00015\n",
      "Train Epoch:  14 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.51892\tLoss_cls_domain: 1.03030\tLoss_cls_source: 0.14801\tLoss_cls_target: 0.03353\tLoss_global: 30.04008\tLoss_local: 0.00008\n",
      "Train Epoch:  14 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.53823\tLoss_cls_domain: 0.45396\tLoss_cls_source: 0.14096\tLoss_cls_target: 0.03490\tLoss_global: 24.91903\tLoss_local: 0.00012\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1462/2272 (64.3486%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  15\n",
      "Train Epoch:  15 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.53263\tLoss_cls_domain: 1.44734\tLoss_cls_source: 0.23970\tLoss_cls_target: 0.03785\tLoss_global: 46.42315\tLoss_local: 0.00026\n",
      "Train Epoch:  15 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.52392\tLoss_cls_domain: 3.78706\tLoss_cls_source: 0.09298\tLoss_cls_target: 0.03731\tLoss_global: 50.10349\tLoss_local: 0.00015\n",
      "Train Epoch:  15 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.51054\tLoss_cls_domain: 1.95810\tLoss_cls_source: 0.11148\tLoss_cls_target: 0.03089\tLoss_global: 52.70599\tLoss_local: 0.00010\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1501/2272 (66.0651%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  16\n",
      "Train Epoch:  16 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.51901\tLoss_cls_domain: 1.25783\tLoss_cls_source: 0.13518\tLoss_cls_target: 0.03001\tLoss_global: 64.30106\tLoss_local: 0.00010\n",
      "Train Epoch:  16 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.52737\tLoss_cls_domain: 1.41896\tLoss_cls_source: 0.15749\tLoss_cls_target: 0.02401\tLoss_global: 25.36691\tLoss_local: 0.00007\n",
      "Train Epoch:  16 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.49707\tLoss_cls_domain: 1.67626\tLoss_cls_source: 0.13638\tLoss_cls_target: 0.02254\tLoss_global: 27.14540\tLoss_local: 0.00027\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1376/2272 (60.5634%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  17\n",
      "Train Epoch:  17 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.50351\tLoss_cls_domain: 1.30115\tLoss_cls_source: 0.09213\tLoss_cls_target: 0.02555\tLoss_global: 51.80726\tLoss_local: 0.00014\n",
      "Train Epoch:  17 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.52140\tLoss_cls_domain: 3.95496\tLoss_cls_source: 0.13094\tLoss_cls_target: 0.02548\tLoss_global: 22.38367\tLoss_local: 0.00039\n",
      "Train Epoch:  17 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.51817\tLoss_cls_domain: 1.33422\tLoss_cls_source: 0.12945\tLoss_cls_target: 0.02788\tLoss_global: 49.10712\tLoss_local: 0.00032\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1361/2272 (59.9032%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  18\n",
      "Train Epoch:  18 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.52385\tLoss_cls_domain: 5.70295\tLoss_cls_source: 0.16407\tLoss_cls_target: 0.02659\tLoss_global: 32.12066\tLoss_local: 0.00019\n",
      "Train Epoch:  18 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.52497\tLoss_cls_domain: 1.10151\tLoss_cls_source: 0.13534\tLoss_cls_target: 0.02911\tLoss_global: 75.58724\tLoss_local: 0.00013\n",
      "Train Epoch:  18 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.50746\tLoss_cls_domain: 2.54054\tLoss_cls_source: 0.13253\tLoss_cls_target: 0.02695\tLoss_global: 28.85153\tLoss_local: 0.00022\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1288/2272 (56.6901%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  19\n",
      "Train Epoch:  19 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.48864\tLoss_cls_domain: 1.03285\tLoss_cls_source: 0.14380\tLoss_cls_target: 0.02424\tLoss_global: 30.67995\tLoss_local: 0.00022\n",
      "Train Epoch:  19 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.51755\tLoss_cls_domain: 1.39064\tLoss_cls_source: 0.10909\tLoss_cls_target: 0.02787\tLoss_global: 58.14902\tLoss_local: 0.00010\n",
      "Train Epoch:  19 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.51921\tLoss_cls_domain: 1.07677\tLoss_cls_source: 0.14824\tLoss_cls_target: 0.02341\tLoss_global: 51.63580\tLoss_local: 0.00015\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1289/2272 (56.7342%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  20\n",
      "Train Epoch:  20 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.51067\tLoss_cls_domain: 1.55073\tLoss_cls_source: 0.13047\tLoss_cls_target: 0.02277\tLoss_global: 46.44991\tLoss_local: 0.00029\n",
      "Train Epoch:  20 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.50699\tLoss_cls_domain: 2.27953\tLoss_cls_source: 0.14681\tLoss_cls_target: 0.02194\tLoss_global: 48.70468\tLoss_local: 0.00012\n",
      "Train Epoch:  20 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.51568\tLoss_cls_domain: 6.03493\tLoss_cls_source: 0.14014\tLoss_cls_target: 0.02023\tLoss_global: 70.77211\tLoss_local: 0.00017\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1264/2272 (55.6338%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  21\n",
      "Train Epoch:  21 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.50447\tLoss_cls_domain: 5.32701\tLoss_cls_source: 0.14680\tLoss_cls_target: 0.01551\tLoss_global: 236.00806\tLoss_local: 0.00046\n",
      "Train Epoch:  21 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.50992\tLoss_cls_domain: 1.62864\tLoss_cls_source: 0.14740\tLoss_cls_target: 0.01813\tLoss_global: 68.33461\tLoss_local: 0.00025\n",
      "Train Epoch:  21 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.52133\tLoss_cls_domain: 0.56910\tLoss_cls_source: 0.19791\tLoss_cls_target: 0.03130\tLoss_global: 214.63483\tLoss_local: 0.00037\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1255/2272 (55.2377%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  22\n",
      "Train Epoch:  22 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.49560\tLoss_cls_domain: 3.71688\tLoss_cls_source: 0.13412\tLoss_cls_target: 0.02828\tLoss_global: 195.22595\tLoss_local: 0.00018\n",
      "Train Epoch:  22 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.51094\tLoss_cls_domain: 3.68201\tLoss_cls_source: 0.41395\tLoss_cls_target: 0.02218\tLoss_global: 60.39119\tLoss_local: 0.00030\n",
      "Train Epoch:  22 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.50864\tLoss_cls_domain: 7.49617\tLoss_cls_source: 0.18681\tLoss_cls_target: 0.01890\tLoss_global: 226.23587\tLoss_local: 0.00025\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1268/2272 (55.8099%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  23\n",
      "Train Epoch:  23 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.53137\tLoss_cls_domain: 4.13428\tLoss_cls_source: 0.15404\tLoss_cls_target: 0.02501\tLoss_global: 25.61076\tLoss_local: 0.00030\n",
      "Train Epoch:  23 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.50511\tLoss_cls_domain: 0.95511\tLoss_cls_source: 0.14487\tLoss_cls_target: 0.02044\tLoss_global: 45.01350\tLoss_local: 0.00029\n",
      "Train Epoch:  23 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.50573\tLoss_cls_domain: 3.42844\tLoss_cls_source: 0.15075\tLoss_cls_target: 0.01735\tLoss_global: 25.27166\tLoss_local: 0.00038\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1263/2272 (55.5898%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  24\n",
      "Train Epoch:  24 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.52629\tLoss_cls_domain: 2.54778\tLoss_cls_source: 0.21653\tLoss_cls_target: 0.02005\tLoss_global: 215.25388\tLoss_local: 0.00050\n",
      "Train Epoch:  24 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.51976\tLoss_cls_domain: 13.10372\tLoss_cls_source: 0.12377\tLoss_cls_target: 0.02449\tLoss_global: 39.48703\tLoss_local: 0.00027\n",
      "Train Epoch:  24 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.50775\tLoss_cls_domain: 1.67059\tLoss_cls_source: 0.12576\tLoss_cls_target: 0.02957\tLoss_global: 214.57669\tLoss_local: 0.00016\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1265/2272 (55.6778%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  25\n",
      "Train Epoch:  25 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.52032\tLoss_cls_domain: 86.81682\tLoss_cls_source: 0.16047\tLoss_cls_target: 0.02796\tLoss_global: 214.42099\tLoss_local: 0.00013\n",
      "Train Epoch:  25 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.54201\tLoss_cls_domain: 5.10943\tLoss_cls_source: 0.18239\tLoss_cls_target: 0.02508\tLoss_global: 42.53559\tLoss_local: 0.00027\n",
      "Train Epoch:  25 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.49870\tLoss_cls_domain: 65.09199\tLoss_cls_source: 0.20335\tLoss_cls_target: 0.03235\tLoss_global: 44.16179\tLoss_local: 0.00021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1252/2272 (55.1056%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  26\n",
      "Train Epoch:  26 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.52248\tLoss_cls_domain: 0.48601\tLoss_cls_source: 0.16897\tLoss_cls_target: 0.03402\tLoss_global: 213.81027\tLoss_local: 0.00020\n",
      "Train Epoch:  26 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.49800\tLoss_cls_domain: 4.23266\tLoss_cls_source: 0.34660\tLoss_cls_target: 0.03562\tLoss_global: 237.95834\tLoss_local: 0.00030\n",
      "Train Epoch:  26 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.50957\tLoss_cls_domain: 12.77124\tLoss_cls_source: 0.21157\tLoss_cls_target: 0.03276\tLoss_global: 391.33337\tLoss_local: 0.00022\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1272/2272 (55.9859%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  27\n",
      "Train Epoch:  27 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.50510\tLoss_cls_domain: 18.47084\tLoss_cls_source: 0.37086\tLoss_cls_target: 0.02963\tLoss_global: 65.03165\tLoss_local: 0.00013\n",
      "Train Epoch:  27 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.50482\tLoss_cls_domain: 4.63840\tLoss_cls_source: 0.33565\tLoss_cls_target: 0.03078\tLoss_global: 64.35392\tLoss_local: 0.00017\n",
      "Train Epoch:  27 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.51032\tLoss_cls_domain: 0.96899\tLoss_cls_source: 0.34864\tLoss_cls_target: 0.02469\tLoss_global: 43.13692\tLoss_local: 0.00020\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1350/2272 (59.4190%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  28\n",
      "Train Epoch:  28 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.51776\tLoss_cls_domain: 2.02860\tLoss_cls_source: 0.24901\tLoss_cls_target: 0.03035\tLoss_global: 24.07796\tLoss_local: 0.00023\n",
      "Train Epoch:  28 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.50339\tLoss_cls_domain: 26.28997\tLoss_cls_source: 0.23698\tLoss_cls_target: 0.02538\tLoss_global: 44.16756\tLoss_local: 0.00036\n",
      "Train Epoch:  28 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.51716\tLoss_cls_domain: 78.61703\tLoss_cls_source: 0.22412\tLoss_cls_target: 0.02051\tLoss_global: 59.50512\tLoss_local: 0.00027\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1237/2272 (54.4454%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  29\n",
      "Train Epoch:  29 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.51402\tLoss_cls_domain: 5.03979\tLoss_cls_source: 0.32675\tLoss_cls_target: 0.02228\tLoss_global: 25.40626\tLoss_local: 0.00019\n",
      "Train Epoch:  29 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.49595\tLoss_cls_domain: 1.58330\tLoss_cls_source: 0.28422\tLoss_cls_target: 0.02489\tLoss_global: 58.98283\tLoss_local: 0.00050\n",
      "Train Epoch:  29 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.51433\tLoss_cls_domain: 2.21424\tLoss_cls_source: 0.37931\tLoss_cls_target: 0.03013\tLoss_global: 44.39459\tLoss_local: 0.00042\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1254/2272 (55.1937%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  30\n",
      "Train Epoch:  30 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.51689\tLoss_cls_domain: 1.68743\tLoss_cls_source: 0.33184\tLoss_cls_target: 0.03652\tLoss_global: 417.16766\tLoss_local: 0.00020\n",
      "Train Epoch:  30 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.51124\tLoss_cls_domain: 5.42938\tLoss_cls_source: 0.43731\tLoss_cls_target: 0.03848\tLoss_global: 40.09455\tLoss_local: 0.00014\n",
      "Train Epoch:  30 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.52029\tLoss_cls_domain: 3.63362\tLoss_cls_source: 0.36417\tLoss_cls_target: 0.04156\tLoss_global: 197.70370\tLoss_local: 0.00026\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1241/2272 (54.6215%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  31\n",
      "Train Epoch:  31 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.50665\tLoss_cls_domain: 17.82368\tLoss_cls_source: 0.34388\tLoss_cls_target: 0.04809\tLoss_global: 542921.18750\tLoss_local: 0.00038\n",
      "Train Epoch:  31 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.49779\tLoss_cls_domain: 2.50022\tLoss_cls_source: 0.36669\tLoss_cls_target: 0.04096\tLoss_global: 252.88013\tLoss_local: 0.00032\n",
      "Train Epoch:  31 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.52237\tLoss_cls_domain: 2.00942\tLoss_cls_source: 0.40642\tLoss_cls_target: 0.03393\tLoss_global: 556.25989\tLoss_local: 0.00032\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 931/2272 (40.9771%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  32\n",
      "Train Epoch:  32 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.51217\tLoss_cls_domain: 90.32160\tLoss_cls_source: 0.32191\tLoss_cls_target: 0.03145\tLoss_global: 57.69825\tLoss_local: 0.00029\n",
      "Train Epoch:  32 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.50795\tLoss_cls_domain: 4.45983\tLoss_cls_source: 0.29077\tLoss_cls_target: 0.04130\tLoss_global: 368.05585\tLoss_local: 0.00028\n",
      "Train Epoch:  32 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.50765\tLoss_cls_domain: 6.93827\tLoss_cls_source: 0.29697\tLoss_cls_target: 0.03424\tLoss_global: 39.97459\tLoss_local: 0.00040\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1118/2272 (49.2077%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  33\n",
      "Train Epoch:  33 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.51432\tLoss_cls_domain: 31.23431\tLoss_cls_source: 0.42715\tLoss_cls_target: 0.03935\tLoss_global: 220.53358\tLoss_local: 0.00048\n",
      "Train Epoch:  33 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.51981\tLoss_cls_domain: 14.34312\tLoss_cls_source: 0.48959\tLoss_cls_target: 0.04072\tLoss_global: 50.79797\tLoss_local: 0.00033\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 971/2272 (42.7377%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  34\n",
      "Train Epoch:  34 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.50048\tLoss_cls_domain: 79.65307\tLoss_cls_source: 0.50280\tLoss_cls_target: 0.03565\tLoss_global: 221.40591\tLoss_local: 0.00026\n",
      "Train Epoch:  34 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.51862\tLoss_cls_domain: 2.53687\tLoss_cls_source: 0.54864\tLoss_cls_target: 0.03568\tLoss_global: 10292.76855\tLoss_local: 0.00032\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 911/2272 (40.0968%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  35\n",
      "Train Epoch:  35 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.50466\tLoss_cls_domain: 13.37106\tLoss_cls_source: 0.53381\tLoss_cls_target: 0.04112\tLoss_global: 240.41023\tLoss_local: 0.00064\n",
      "Train Epoch:  35 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.50760\tLoss_cls_domain: 2.97292\tLoss_cls_source: 0.59575\tLoss_cls_target: 0.03417\tLoss_global: 365.23770\tLoss_local: 0.00023\n",
      "Train Epoch:  35 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.50720\tLoss_cls_domain: 58.01692\tLoss_cls_source: 0.46408\tLoss_cls_target: 0.04149\tLoss_global: 382.98355\tLoss_local: 0.00048\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 935/2272 (41.1532%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  36\n",
      "Train Epoch:  36 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.50423\tLoss_cls_domain: 38.38916\tLoss_cls_source: 0.44339\tLoss_cls_target: 0.04650\tLoss_global: 516.21777\tLoss_local: 0.00085\n",
      "Train Epoch:  36 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.50580\tLoss_cls_domain: 8.11553\tLoss_cls_source: 0.59274\tLoss_cls_target: 0.04586\tLoss_global: 10400.22461\tLoss_local: 0.00048\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1080/2272 (47.5352%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  37\n",
      "Train Epoch:  37 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.50045\tLoss_cls_domain: 1.22145\tLoss_cls_source: 0.61514\tLoss_cls_target: 0.04119\tLoss_global: 233.46815\tLoss_local: 0.00039\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1153/2272 (50.7482%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  38\n",
      "Train Epoch:  38 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.51973\tLoss_cls_domain: 6.92656\tLoss_cls_source: 0.60278\tLoss_cls_target: 0.03488\tLoss_global: 20832.55469\tLoss_local: 0.00027\n",
      "Train Epoch:  38 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.51954\tLoss_cls_domain: 4.47956\tLoss_cls_source: 0.74900\tLoss_cls_target: 0.03567\tLoss_global: 10360.74707\tLoss_local: 0.00067\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 171/2272 (7.5264%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  39\n",
      "Train Epoch:  39 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.50924\tLoss_cls_domain: 765.19202\tLoss_cls_source: 0.78751\tLoss_cls_target: 0.03114\tLoss_global: 9923.49805\tLoss_local: 0.00031\n",
      "Train Epoch:  39 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.50219\tLoss_cls_domain: 2.10788\tLoss_cls_source: 0.99557\tLoss_cls_target: 0.02657\tLoss_global: 28445128.00000\tLoss_local: 0.00093\n",
      "Train Epoch:  39 [  5/7.80859375 (0.64%)]\tLoss_zinb: 0.50458\tLoss_cls_domain: 3.35815\tLoss_cls_source: 1.23509\tLoss_cls_target: 0.03811\tLoss_global: 23.86190\tLoss_local: 0.00082\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 917/2272 (40.3609%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  40\n",
      "Train Epoch:  40 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.53037\tLoss_cls_domain: 6.13448\tLoss_cls_source: 1.00563\tLoss_cls_target: 0.03620\tLoss_global: 20793.35742\tLoss_local: 0.00062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 906/2272 (39.8768%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  41\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 896/2272 (39.4366%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  42\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 916/2272 (40.3169%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  43\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 903/2272 (39.7447%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  44\n",
      "Train Epoch:  44 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.53865\tLoss_cls_domain: 32.54168\tLoss_cls_source: 0.92785\tLoss_cls_target: 0.03679\tLoss_global: 30898.53516\tLoss_local: 0.00087\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 935/2272 (41.1532%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  45\n",
      "Train Epoch:  45 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.51374\tLoss_cls_domain: 25.66884\tLoss_cls_source: 0.90824\tLoss_cls_target: 0.03036\tLoss_global: 196.67778\tLoss_local: 0.00035\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 262/2272 (11.5317%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  46\n",
      "Train Epoch:  46 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.52541\tLoss_cls_domain: 46.74648\tLoss_cls_source: 0.78236\tLoss_cls_target: 0.02679\tLoss_global: 20964.84180\tLoss_local: 0.00046\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 241/2272 (10.6074%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  47\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 230/2272 (10.1232%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  48\n",
      "Train Epoch:  48 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.51759\tLoss_cls_domain: 2.70943\tLoss_cls_source: 0.78470\tLoss_cls_target: 0.02217\tLoss_global: 10399.72266\tLoss_local: 0.00026\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 161/2272 (7.0863%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  49\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 166/2272 (7.3063%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  50\n",
      "Train Epoch:  50 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.51789\tLoss_cls_domain: 103.44427\tLoss_cls_source: 0.77145\tLoss_cls_target: 0.01990\tLoss_global: 28455308.00000\tLoss_local: 0.00012\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 141/2272 (6.2060%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  51\n",
      "Train Epoch:  51 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.50964\tLoss_cls_domain: 799.74054\tLoss_cls_source: 0.93760\tLoss_cls_target: 0.01787\tLoss_global: 45.54592\tLoss_local: 0.00012\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 128/2272 (5.6338%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  52\n",
      "Train Epoch:  52 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.52304\tLoss_cls_domain: 7.67073\tLoss_cls_source: 1.00928\tLoss_cls_target: 0.01992\tLoss_global: 400.24356\tLoss_local: 0.00018\n",
      "Train Epoch:  52 [  3/7.80859375 (0.38%)]\tLoss_zinb: 0.51849\tLoss_cls_domain: 60.28314\tLoss_cls_source: 1.13828\tLoss_cls_target: 0.02337\tLoss_global: 10240.36914\tLoss_local: 0.00012\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 132/2272 (5.8099%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  53\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 124/2272 (5.4577%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  54\n",
      "Train Epoch:  54 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.52572\tLoss_cls_domain: 23.69255\tLoss_cls_source: 0.88054\tLoss_cls_target: 0.01731\tLoss_global: 10570.89844\tLoss_local: 0.00104\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 130/2272 (5.7218%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  55\n",
      "Train Epoch:  55 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.49976\tLoss_cls_domain: 8.14013\tLoss_cls_source: 0.80851\tLoss_cls_target: 0.01538\tLoss_global: 10239.16895\tLoss_local: 0.00057\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 129/2272 (5.6778%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  56\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 141/2272 (6.2060%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  57\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 135/2272 (5.9419%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  58\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 131/2272 (5.7658%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  59\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 128/2272 (5.6338%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  60\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 130/2272 (5.7218%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  61\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 141/2272 (6.2060%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  62\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 136/2272 (5.9859%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  63\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 137/2272 (6.0299%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  64\n",
      "Train Epoch:  64 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.53082\tLoss_cls_domain: 9.99183\tLoss_cls_source: 0.71007\tLoss_cls_target: 0.01758\tLoss_global: 26.14119\tLoss_local: 0.00019\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 142/2272 (6.2500%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  65\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 150/2272 (6.6021%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  66\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 140/2272 (6.1620%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  67\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 142/2272 (6.2500%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  68\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 145/2272 (6.3820%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  69\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 147/2272 (6.4701%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  70\n",
      "Train Epoch:  70 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.53909\tLoss_cls_domain: 8.35665\tLoss_cls_source: 0.70663\tLoss_cls_target: 0.01996\tLoss_global: 27.03949\tLoss_local: 0.00171\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 300/2272 (13.2042%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  71\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 299/2272 (13.1602%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  72\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 292/2272 (12.8521%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  73\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 287/2272 (12.6320%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  74\n",
      "Train Epoch:  74 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.50982\tLoss_cls_domain: 10.21451\tLoss_cls_source: 0.61601\tLoss_cls_target: 0.01819\tLoss_global: 28485758.00000\tLoss_local: 0.00056\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 381/2272 (16.7694%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  75\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 370/2272 (16.2852%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  76\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 366/2272 (16.1092%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  77\n",
      "Train Epoch:  77 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.50993\tLoss_cls_domain: 8.52799\tLoss_cls_source: 0.70713\tLoss_cls_target: 0.01524\tLoss_global: 20139.68945\tLoss_local: 0.00049\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 424/2272 (18.6620%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  78\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 412/2272 (18.1338%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  79\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 411/2272 (18.0898%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  80\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 421/2272 (18.5299%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  81\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 410/2272 (18.0458%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  82\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 427/2272 (18.7940%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  83\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 420/2272 (18.4859%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  84\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 411/2272 (18.0898%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  85\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 413/2272 (18.1778%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  86\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 409/2272 (18.0018%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  87\n",
      "Train Epoch:  87 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.53012\tLoss_cls_domain: 124.62927\tLoss_cls_source: 0.65756\tLoss_cls_target: 0.01661\tLoss_global: 28465056.00000\tLoss_local: 0.00045\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 377/2272 (16.5933%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  88\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 369/2272 (16.2412%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch:  89 [  1/7.80859375 (0.13%)]\tLoss_zinb: 0.51136\tLoss_cls_domain: 7.18123\tLoss_cls_source: 0.66595\tLoss_cls_target: 0.01592\tLoss_global: 28465064.00000\tLoss_local: 0.00170\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 326/2272 (14.3486%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  90\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 334/2272 (14.7007%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  91\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 327/2272 (14.3926%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  92\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 330/2272 (14.5246%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  93\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 342/2272 (15.0528%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  94\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 337/2272 (14.8327%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  95\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 341/2272 (15.0088%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  96\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 331/2272 (14.5687%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  97\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 346/2272 (15.2289%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  98\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 337/2272 (14.8327%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n",
      "Epoch:  99\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 329/2272 (14.4806%), Best Accuracy: 1551/2272 (68.2658%)  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in range(100):\n",
    "    print('Epoch: ', t)\n",
    "    num = solver.train_gcn_adapt(t)\n",
    "    bestacc = solver.test(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "[ 5  6  8  9 11 12]\n",
      "[ 5  6  7  8  9 11 12]\n",
      "[ 2  5  6  7  8  9 10 11 12]\n",
      "[ 5  6  7 12]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "[ 5  6  8  9 11 12]\n",
      "[ 5  6  7  8  9 11 12]\n",
      "[ 2  5  6  7  8  9 10 11 12]\n",
      "[ 5  6  7 12]\n"
     ]
    }
   ],
   "source": [
    "solver.test_feature(\"/data/msyuan/results/scMSDA/Ablation/use_target_1_ndomain_3/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "[ 5  6  8  9 11 12]\n",
      "[ 5  6  7  8  9 11 12]\n",
      "[ 2  5  6  7  8  9 10 11 12]\n",
      "[ 5  6  7 12]\n"
     ]
    }
   ],
   "source": [
    "solver.latent_feature(\"/data/msyuan/results/scMSDA/multi_source_insufficient_b_4/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(bestacc[0])\n",
    "true = pd.DataFrame(bestacc[1])\n",
    "pred.to_csv(\"/data/msyuan/results/scMSDA/multisource_insufficient_scMSDA_pred.csv\")\n",
    "true.to_csv(\"/data/msyuan/results/scMSDA/multisource_insufficient_scMSDA_true.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
